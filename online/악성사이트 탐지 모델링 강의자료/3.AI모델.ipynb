{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTku+v8S53RhAcYUyEVfM+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"EE1NEO7bWVLy"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","\n","from aicentro.session import Session\n","sacp_session = Session(verify=False)\n","from aicentro.framework.framework import BaseFramework as SacpFrm\n","sacp_framework = SacpFrm(session=sacp_session)"]},{"cell_type":"markdown","source":["###  <b>데이터 불러오기</b>\n","---\n","- 학습 데이터 불러오기"],"metadata":{"id":"Q6mIuF2nWYle"}},{"cell_type":"code","source":["Train_Data = pd.read_csv('TrainData.csv',delimiter=',')"],"metadata":{"id":"Z5M3oh-4WZab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <b>데이터 전처리</b>\n","---"],"metadata":{"id":"ZQKZ5zWiWaqG"}},{"cell_type":"code","source":["Train_Data=Train_Data.drop_duplicates()"],"metadata":{"id":"vsnI24YQWbp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Train_Data['Result_v1'].replace({'benign':1,'malicious':-1}, inplace=True)"],"metadata":{"id":"sSQgEqjiWcm5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Train_Data.drop(columns=[\"url_chinese_present\",\"html_num_tags('applet')\"],inplace=True)"],"metadata":{"id":"Hq-EyVvpWdhJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Train_Data = Train_Data.dropna(axis=0)"],"metadata":{"id":"v022ZSoGWelf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = Train_Data.iloc[:,0:len(Train_Data.columns)-1].values\n","y = Train_Data.iloc[:,len(Train_Data.columns)-1].values"],"metadata":{"id":"6ahibzVEWfbC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <b> train_test_split을 이용하여, train_x, val_x, train_y, val_y로 데이터 분리</b>\n","---\n","\n","- test_size = 0.3\n","- random_state = 2021"],"metadata":{"id":"dOlX_y9TWgwm"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"GmUHYf6PWhxQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train_test_split 사용\n","train_x, val_x, train_y, val_y = train_test_split(X, y,test_size=0.3,random_state=2021)"],"metadata":{"id":"DQaG9EEYWjZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x.shape, val_x.shape, train_y.shape, val_y.shape"],"metadata":{"id":"61GD9c-TWkw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <b>Confusion Matrix 함수 정의</b>\n","---"],"metadata":{"id":"zS6F_7HQWl0e"}},{"cell_type":"markdown","source":["- Confusion Matrix란?\n"," - Training 을 통한 Prediction 성능을 측정하기 위해 예측 value와 실제 value를 비교하기 위한 표\n"," - 참고 사이트 : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",""],"metadata":{"id":"0ZTuXdXzWmy2"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report as creport\n","from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"],"metadata":{"id":"eZGZlpTLWoHs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_confusion_matrix(ax, matrix, labels = ['malicious','benign'], title='Confusion matrix', fontsize=9):\n","    ax.set_xticks([x for x in range(len(labels))])\n","    ax.set_yticks([y for y in range(len(labels))])\n","\n","    # Place labels on minor ticks\n","    ax.set_xticks([x + 0.5 for x in range(len(labels))], minor=True)\n","    ax.set_xticklabels(labels, rotation='90', fontsize=fontsize, minor=True)\n","    ax.set_yticks([y + 0.5 for y in range(len(labels))], minor=True)\n","    ax.set_yticklabels(labels[::-1], fontsize=fontsize, minor=True)\n","\n","    # Hide major tick labels\n","    ax.tick_params(which='major', labelbottom='off', labelleft='off')\n","\n","    # Finally, hide minor tick marks\n","    ax.tick_params(which='minor', width=0)\n","\n","    # Plot heat map\n","    proportions = [1. * row / sum(row) for row in matrix]\n","    ax.pcolor(np.array(proportions[::-1]), cmap=plt.cm.Blues)\n","\n","    # Plot counts as text\n","    for row in range(len(matrix)):\n","        for col in range(len(matrix[row])):\n","            confusion = matrix[::-1][row][col]\n","            if confusion != 0:\n","                ax.text(col + 0.5, row + 0.5, int(confusion),\n","                        fontsize=fontsize,\n","                        horizontalalignment='center',\n","                        verticalalignment='center')\n","\n","    # Add finishing touches\n","    ax.grid(True, linestyle=':')\n","    ax.set_title(title, fontsize=fontsize)\n","    ax.set_xlabel('prediction', fontsize=fontsize)\n","    ax.set_ylabel('actual', fontsize=fontsize)\n","\n","    plt.show()"],"metadata":{"id":"uMSIvi4mWptj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 샘플\n","##### > confusion = confusion_matrix(test_y, dt_pred)\n","##### > fig, ax = plt.subplots(figsize=(10,3))\n","##### > plot_confusion_matrix(ax, confusion, fontsize=30)"],"metadata":{"id":"LdplBIBXWrEE"}},{"cell_type":"markdown","source":["### <b> Q2. DecisonTree 모델을 만들어보자\n","---\n","  \n","* DecisionTree란? <br>\n","    * 의사결정 규칙을 나무구조로 나타내어 전체 데이터를 소집단으로 분류하거나 예측<br>\n","    * 참고 사이트 : https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n","\n","    * 예) Iris DecisionTree(결정트리)"],"metadata":{"id":"5UiTzt9EWsjT"}},{"cell_type":"markdown","source":["![DecisionTree_s.png](attachment:9d7f41bd-2872-4b7e-afe2-d6c4f7b968ec.png)<br>\n","> 출처 : 핸즈온 머신러닝 2판 | 한빛미디어 | 오렐리앙제롱 지음, 박해선 옮김|\n","\n","\n","* 주요 하이퍼 파라미터<br>\n","<table align=\"left\">\n","    <tr>\n","        <td align=\"center\">파라미터 명</td><td align=\"center\">설명</td>\n","    </tr>\n","     <tr>\n","        <td align=\"center\">min_samples_split</td><td>노드를 분할하기 위한 최소한의 샘플 데이터 수,  default=2</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">min_samples_leaf</td><td>말단 노드가 되기 위한 최소한의 샘플 데이터</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_features</td><td>최적의 분할을 위해 고려할 최대 feature 개수, default = None(데이터 세트의 모든 피처를 사용)</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_depth</td><td>트리의 최대 깊이, default=None(완벽하게 클래스 값이 결정될 때 까지 계속)</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_leaf_nodes</td><td>말단 노드의 최대 개수</td>\n","    </tr>\n","</table>"],"metadata":{"id":"whDFo1FvWz8t"}},{"cell_type":"code","source":["# 1. import\n","from sklearn.tree import DecisionTreeClassifier"],"metadata":{"id":"DjctVHy7WyXW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.선언\n","dtc = DecisionTreeClassifier()"],"metadata":{"id":"BvhJ7ORvW2fJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. fit()\n","dtc.fit(train_x,train_y)\n"],"metadata":{"id":"w2fSWpWFW3V_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. predict()\n","dtc_pred = dtc.predict(val_x)\n"],"metadata":{"id":"h3T0bM6bW513"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 및 val 데이터 정확도 확인 : score()\n","dtc.score(train_x, train_y), dtc.score(val_x, val_y)"],"metadata":{"id":"s3uOs7qgW7Dc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Confusion Matrix 확인\n","confusion = confusion_matrix(val_y, dtc_pred)\n","fig, ax = plt.subplots(figsize=(10,3))\n","plot_confusion_matrix(ax, confusion, fontsize=30)\n"],"metadata":{"id":"lAq6bwMfW77F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(20,12))\n","plt.barh(y=Train_Data.columns[:-1],\n","        width = dtc.feature_importances_)\n","plt.show()"],"metadata":{"id":"FDxoiKE6W9JH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","### <b>앙상블(Ensemble)</b>\n","* 앙상블(Ensemble)이란? <br>\n","   * 여러 개의 결정 트리(Decision Tree)를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신러닝 기법<br>\n","   * 앙상블 학습법에는 배깅(Bagging), 부스팅(Boosting), 스태킹(Staking) 등이 존재<br><br>\n","* 배깅(Bagging)이란? <br>\n","   * Bagging은 Bootstrap(데이터를 만드는 방법) + Aggregation(의사결정방법)의 약자\n","   * 데이터를 랜덤으로 복원추출(샘플링)하여 합한 결과 기반으로 의사결정하는 방법 <br>\n","![ebsemble_1.png](attachment:663729da-0ab4-45ec-a4cd-7febcbcb214a.png)\n","> 출처 : swallow.github.io\n","\n","* 부스팅(Boosting)이란? <br>\n","   * 약한 학습기를 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법\n","   * 부스팅 방법의 아이디어는 앞의모델을 보완해나가면서 일련의 예측기를 학습시키는 과정<br>\n","![ebsemble_2.png](attachment:ba7ca11a-7f06-4dbf-a106-35fb8a5b1993.png)\n","> 출처: Medium (Boosting and Bagging explained with examples)\n","* 배깅(Bagging)과 부스팅(Boosting) 차이 <br>\n","   * 배깅(Bagging)은 병렬로 학습하는 반면, 부스팅(Boosting)은 순차적으로 학습<br>\n","![ebsemble_4.png](attachment:cff8740a-def9-4236-b06b-63c5a1962712.png)\n","> 출처 : https://assaeunji.github.io/ml/2020-08-06-tree/"],"metadata":{"id":"XVl2di8-W-ic"}},{"cell_type":"markdown","source":["## Q3. Random Forest 모델을 만들어보자\n","---\n","* RandomForest란? <br>\n","    * 여러 개의 DecisionTree(결정트리)를 활용한 배깅 방식의 대표적인 알고리즘<br>\n","    * 배깅과 페이스팅은 훈련 세트에서 무작위로 샘플링하여 여러 개의 예측기를 훈련<br>\n","    * 참고 사이트 : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n","\n","\n","\n","![RandomForest_1.png](attachment:b4dfa600-3df4-426f-a264-387564f09df1.png)\n","> 출처 : https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d"],"metadata":{"id":"O5G3G5WfXAGq"}},{"cell_type":"markdown","source":["* 주요 하이퍼 파라미터<br>\n","<table align=\"left\">\n","    <tr>\n","        <td align=\"center\">파라미터 명</td><td align=\"center\">설명</td>\n","    </tr>\n","     <tr>\n","        <td align=\"center\">n_estimators</td><td>생성할 의사결정 나무 개수,  default=10</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">min_samples_split</td><td>노드를 분할하기 위한 최소한의 샘플 데이터 수, default=2</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">min_samples_leaf</td><td>리프노드가 되기 위해 필요한 최소한의 샘플 데이터 수</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_features</td><td>의사결정 나무 만들시에 사용하는 최대 feature 개수, default=auto</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_depth</td><td>트리의 최대 깊이, default=None</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">max_leaf_nodes</td><td>리프노드의 최대 개수</td>\n","    </tr>\n","</table>"],"metadata":{"id":"wLHqiNZHXCJs"}},{"cell_type":"code","source":["# 1. import\n","from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"iobWlaj7XDUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.선언\n","rfc = RandomForestClassifier()"],"metadata":{"id":"FllXjno_XEL8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. fit()\n","rfc.fit(train_x,train_y)"],"metadata":{"id":"k4llq06RXE9F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. predict()\n","rfc_pred = rfc.predict(val_x)"],"metadata":{"id":"HMX34UIZXF2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 및 val 데이터 정확도 확인 :score()\n","rfc.score(train_x,train_y), rfc.score(val_x, val_y)"],"metadata":{"id":"LJPAuFU-XGlj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix 확인\n","confusion = confusion_matrix(val_y, rfc_pred)\n","fig, ax = plt.subplots(figsize=(10,3))\n","plot_confusion_matrix(ax, confusion, fontsize=30)"],"metadata":{"id":"L2Bb5TRjXHm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature_importances_ 활용 Feature별 가중치 확인\n","plt.figure(figsize=(20,12))\n","plt.barh(y=Train_Data.columns[:-1],\n","        width = rfc.feature_importances_)\n","plt.show()"],"metadata":{"id":"pCVnB6YWXI0J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <b> Q4. AdaBoost 모델을 만들어보자\n","---\n","* AdaBoost란? <br>\n","    * 약한 학습기(weak learner)의 오류 데이터에 가중치를 부여하면서 부스팅을 수행하며 학습<br>\n","    * 참고 사이트 : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html<br>\n","![AdaBoost_2.png](attachment:84e85b66-d020-4a41-bcfd-d22f86103551.png)<br>\n","![AdaBoost_!.png](attachment:ef5e0b8d-1f80-45a0-9fbc-f9e8f622f48e.png)<br>\n","> 출처 : https://devkor.tistory.com/entry/Ensemble-Methods"],"metadata":{"id":"0-Kdk0i_XKfs"}},{"cell_type":"markdown","source":["* 주요 하이퍼 파라미터<br>\n","<table align=\"left\">\n","    <tr>\n","        <td align=\"center\">파라미터 명</td><td align=\"center\">설명</td>\n","    </tr>\n","     <tr>\n","        <td align=\"center\">base_estimators</td><td>학습에 사용하는 알고리즘, default = None(DecisionTreeClassifier 적용)</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">n_estimators</td><td>생성할 약한 학습기의 개수를 지정, default = 50</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">learning_rate</td><td>학습을 진행할 때마다 적용하는 학습률(0~1 사이의 값), default = 1</td>\n","    </tr>\n","</table>"],"metadata":{"id":"uyRq5Ax4XL19"}},{"cell_type":"code","source":["# 1. import\n","from sklearn.ensemble import AdaBoostClassifier\n"],"metadata":{"id":"dA1dRP_pXM09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.선언\n","abc = AdaBoostClassifier()"],"metadata":{"id":"8aZKKgshXNvQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. fit()\n","abc.fit(train_x,train_y)\n"],"metadata":{"id":"T5V9T6BXXOlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. predict()\n","abc_pred = abc.predict(val_x)"],"metadata":{"id":"WasT-WvzXPbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 및 val 데이터 정확도 확인 : score()\n","abc.score(train_x, train_y), abc.score(val_x, val_y)"],"metadata":{"id":"eID9dUaeXQY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Confusion Matrix 확인\n","confusion = confusion_matrix(val_y, abc_pred)\n","fig, ax = plt.subplots(figsize=(10,3))\n","plot_confusion_matrix(ax, confusion, fontsize=30)"],"metadata":{"id":"lW3fKYndXRSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature_importances_ 활용 Feature별 가중치 확인\n","plt.figure(figsize=(20,12))\n","plt.barh(\n","    y=Train_Data.columns[:-1],\n","        width = abc.feature_importances_)\n","plt.show()"],"metadata":{"id":"iK3YBuYQXSRD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### <b> Q5. Gradient Boost 모델을 만들어보자\n","---\n","* GradientBoost란? <br>\n","    * 이전 예측기가 만든 잔여오차에 새로운 예측기를 학습<br>\n","    * Gradient Boost를 발전시킨 모델이 XGBoost, Light GBM, CatBoost<br>\n","    * 참고 사이트 : https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n","\n","![GB_1.png](attachment:fff8ab0c-4bd2-4eea-b472-1fbde5d04daa.png)\n","> 출처 : https://devkor.tistory.com/entry/Ensemble-Methods"],"metadata":{"id":"L63YlHiDXTk-"}},{"cell_type":"markdown","source":["* 주요 하이퍼 파라미터<br>\n","<table align=\"left\">\n","    <tr>\n","        <td align=\"center\">파라미터 명</td><td align=\"center\">설명</td>\n","    </tr>\n","     <tr>\n","        <td align=\"center\">n_estimators</td><td>생성할 트리의 개수, default = 100</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">learning_rate</td><td>학습을 진행할 때마다 적용하는 학습률(0~1 사이의 값), default = 1</td>\n","    </tr>\n","    <tr>\n","        <td align=\"center\">loss</td><td>경사하강법에서 사용할 손실 함수 지정</td>\n","    </tr>\n","</table>"],"metadata":{"id":"bfLeHJxvXU9W"}},{"cell_type":"code","source":["# 1. import\n","from sklearn.ensemble import GradientBoostingClassifier"],"metadata":{"id":"hMgBgSpWXWFd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2.선언\n","gbc = GradientBoostingClassifier()\n"],"metadata":{"id":"1m4j2NgaXXDn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. fit()\n","gbc.fit(train_x,train_y)"],"metadata":{"id":"VZVWYuRZXX2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. predict()\n","gbc_pred = gbc.predict(val_x)"],"metadata":{"id":"gKrnvyG1XYum"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train 및 val 데이터 정확도 확인 : score()\n","gbc.score(train_x, train_y), gbc.score(val_x,val_y)"],"metadata":{"id":"p86MRl0TXZw3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Confusion Matrix 확인\n","confusion = confusion_matrix(val_y, gbc_pred)\n","fig, ax = plt.subplots(figsize=(10,3))\n","plot_confusion_matrix(ax, confusion, fontsize=30)"],"metadata":{"id":"TQjg-iZjXawu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# feature_importances_ 활용 Feature별 가중치 확인\n","plt.figure(figsize=(20,12))\n","plt.barh(y=Train_Data.columns[:-1],\n","        width = gbc.feature_importances_)\n","plt.show()"],"metadata":{"id":"wvA0y8o_Xbxx"},"execution_count":null,"outputs":[]}]}